{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor operation\n",
    "\n",
    "We can build a network by stcaking `Dense` layers on top of each other:\n",
    "\n",
    "`keras.layers.Dense(512, activation='relu')` <=> `output = relu(dot(W, input) + b)`\n",
    "\n",
    "We have three tensor operations here: \n",
    "* a dot product `( dot )` between the input tensor and a tensor named `W`\n",
    "* an addition `( + )` between the resulting 2D tensor and `a` vector `b` \n",
    "* finally, a `relu` operation. `relu(x)` is `max(x, 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise operation\n",
    "\n",
    "* Operations that are applied indpendently to each element in the tensor.\n",
    "* These operations are highly open to _parallel_ implementation or _vectorized_ implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# naive parallel implementation of addition for 2D tensor\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    # avoid overwriting input tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x\n",
    "\n",
    "x = np.random.random([2,2])\n",
    "y = np.random.random([2,2])\n",
    "np.array_equal(naive_add(x, y), (x + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting \n",
    "\n",
    "When two tensors of different lenght are done some operation to together then the smaller tensor is _broadcasted_ to match the shape of the larger tensor. Steps in brpadcasting:\n",
    "\n",
    "1. Axes (called _broadcast axes_) are added to the smaller tensor to match the `ndim` of the larger tensor\n",
    "2. The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.\n",
    "\n",
    "Letâ€™s look at a concrete example. Consider X with shape `(32, 10)` and y with shape\n",
    "`(10,)` . First, we add an empty first axis to y , whose shape becomes (1, 10) . Then, we\n",
    "repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape\n",
    "`(32, 10)` , where `Y[i, :] == y for i in range(0, 32)`.\n",
    "\n",
    "**NOTE**: In terms of implementation, no new 2D tensor is created, because that would be\n",
    "terribly inefficient. The repetition operation is entirely virtual: it happens at the algo-\n",
    "rithmic level rather than at the memory level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17580333 0.14926912]\n",
      " [0.36112706 0.29482549]] \n",
      "\t  +\n",
      " [0.41058159 0.16691853] \n",
      "\t  =\n",
      " [[0.58638493 0.31618765]\n",
      " [0.77170866 0.46174401]]\n"
     ]
    }
   ],
   "source": [
    "# navie implementation of broadcasting\n",
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x\n",
    "\n",
    "x = np.random.random([2,2])\n",
    "y = np.random.random([2])\n",
    "print(x, '\\n\\t  +\\n', y, '\\n\\t  =\\n', naive_add_matrix_and_vector(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor dot\n",
    "\n",
    "The dot operation combines entries in the input tensor which is not equal to the _element-wise multiplication_. Mathematically we write `z = x . y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57124541 0.83774684]\n",
      "[0.56824558 0.79969029]\n",
      "0.9945456900702146\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random(2)\n",
    "y = np.random.random(2)\n",
    "print(\"\\n\".join([str(x), str(y), str(np.dot(x, y))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive implementation of dot product\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z=0\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "np.array_equal(naive_vector_dot(x, y), np.dot(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z\n",
    "\n",
    "mat = np.random.random([2,2])\n",
    "np.array_equal(naive_matrix_vector_dot(mat, y), np.dot(mat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Reshaping\n",
    "\n",
    "* Rehaping a tensor means rearranging its tows and columns to match the target shape. \n",
    "* The reshaped tensor has same total number of coefficents as the initial tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Reshaping: (3, 2)\n",
      "\n",
      "After Reshping: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 1],\n",
    "             [2, 3],\n",
    "             [4, 5]])\n",
    "print(\"Before Reshaping:\", x.shape)\n",
    "\n",
    "x = x.reshape((6, 1))\n",
    "print(\"\\nAfter Reshping:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special case of reshaping that's commonly encountered is _transposition_. _Transposing_ a matrix means exchanging its rows and its columns, so that `x[i, :]` becomes `x[:, i]`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
