{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-based optimization\n",
    "\n",
    "`output = relu(dot(W, input) + b)`\n",
    "\n",
    "* In this expression, `W` and `b` are tensors that are attributes of the layer. They’re called the weights or trainable parameters of the layer (the kernel and bias attributes, respectively).\n",
    "\n",
    "* These  wieghts and biases are assigned randomly in the begining. What comes next is to gradually adjust these weights, based on a feedback signal. This gradual adjustment, also called `training`, is basically the learning that machine learning is all about.\n",
    "\n",
    "Alogrithm of a _training loop_\n",
    "--------------------------------------------\n",
    "1. Draw a batch of training samples x and corresponding targets y.\n",
    "2. Run the network on x (a step called the forward pass) to obtain predictions `y_pred`.\n",
    "3. Compute the loss of the network on the batch, a measure of the mismatch between `y_pred` and y .\n",
    "4. Update all weights of the network in a way that slightly reduces the loss on this batch.\n",
    "\n",
    "We’ll eventually end up with a network that has a very low loss on its training data: a low mismatch between predictions `y_pred` and expected targets y . The network has “learned” to map its inputs to correct targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm of _Stochastic gradient descent_\n",
    "\n",
    "1. Draw a batch of training samples x and corresponding targets y .\n",
    "2. Run the network on x to obtain predictions y_pred .\n",
    "3. Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y .\n",
    "4. Compute the gradient of the loss with regard to the network’s parameters (a backward pass).\n",
    "5. Move the parameters a little in the opposite direction from the gradient—for example W -= step * gradient —thus reducing the loss on the batch a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Step-size by which the minima is found (or we down in the curve) is known as the learning rate. If it’s too small, the descent down the curve will take many iterations, and it could get stuck in a local minimum. If step is too large, your updates may end up taking you to completely random locations on the curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
