{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1. Simple Linear Regression\n",
    "Today we will learn some basics of SLR and implement it in Python in 3 ways:\n",
    "1. From Scratch\n",
    "2. Using Statsmodels \n",
    "3. Using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "##### _\"Our aim is to find the coefficients $b_0$ and $b_1$ from the training dataset (X) and then using those same coefficients we try to predict for the new data (y).\"_\n",
    "\n",
    "The equation of Simple Linear Regression is the linear combination of the input variables (X) with the output variables (y).\n",
    "\n",
    "$${\\hat{y}  =  b_0  +  b_1x}$$\n",
    "\n",
    "$${where, \\hat{y} = Output/Predicted}$$\n",
    "       $${x = Input/Predictor}$$ \n",
    "       $${b_0, b_1 = Coffecients}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 From Scratch\n",
    "\n",
    "Before writing any function let's first see what does out data looks like. We generate a small dataset for this and work with it for simplicity.\n",
    "\n",
    "| X | y |\n",
    "|---|---|\n",
    "| 1 | 1 |\n",
    "| 2 | 3 |\n",
    "| 4 | 3 |\n",
    "| 3 | 2 |\n",
    "| 5 | 5 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEmdJREFUeJzt3X+M5XV97/HnC9ioW60k7txK2B3mj/LHrcYinhAMSUOpaRDJ8oc0oVl/YGom9dqIaRNzdRN7JeGP/mONJZGs0hTrWGnwR1YCTTFIbP8Q7ywuIHfNzf7BwgaSnUIFyXhJVt/3j3NWl7Nndr5n5pw5M58+H8nJ+f54z/m++XDmNd/5zvfsJ1WFJKktF8y6AUnS5BnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAZdNKsD79mzpxYWFmZ1eEnakY4cOfIfVTW3Xt3Mwn1hYYHl5eVZHV6SdqQkJ7rUeVlGkhpkuEtSgwx3SWqQ4S5JDTLcJalBncI9ydNJnkxyNMk5t7ik74tJjid5IsmVk29VktTVOGfuf1hVV1RVb8S+9wKXDx6LwJcm0ZwkNWNpCRYW4IIL+s9LS1M93KTuc78J+Gr15+z7YZKLk1xSVc9P6PUlaedaWoLFRVhd7a+fONFfBzhwYCqH7HrmXsC/JjmSZHHE/kuBZ89aPznYJkk6ePA3wX7G6mp/+5R0PXO/pqqeS/LfgIeS/LSqfnDW/oz4mnNm3h78YFgEmJ+fH7tZSdqRnnlmvO0T0OnMvaqeGzyfAr4NXDVUchLYd9b6XuC5Ea9zqKp6VdWbm1v3n0aQpDasdTI7xZPcdcM9yW8ledOZZeCPgZ8MlR0GPjS4a+Zq4CWvt0vSwB13wO7dr922e3d/+5R0uSzzO8C3k5yp/3pV/UuSPweoqruAB4AbgOPAKvCR6bQrSTvQmT+aHjzYvxQzP98P9in9MRUg/Rtctl6v1yv/VUhJGk+SI2vckv4afkJVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgzuGe5MIkP05y/4h9tyZZSXJ08PjoZNuUJI2jyzR7Z9wGHAN+e43991bVX2y+JUnSZnU6c0+yF3gf8JXptiNJmoSul2W+AHwK+NV5at6f5Ikk9yXZN6ogyWKS5STLKysr4/YqSepo3XBPciNwqqqOnKfsu8BCVb0D+B5wz6iiqjpUVb2q6s3NzW2oYUnS+rqcuV8D7E/yNPAN4LokXzu7oKpeqKpXB6tfBt410S4lSWNZN9yr6tNVtbeqFoBbgIer6gNn1yS55KzV/fT/8CpJmpFx7pZ5jSS3A8tVdRj4RJL9wGngReDWybQnSdqIVNVMDtzr9Wp5eXkmx5aknSrJkarqrVfnJ1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqUOdwT3Jhkh8nuX/EvtcluTfJ8SSPJlmYZJOSpPGMc+Z+G2tPn/dnwH9W1e8Cfwv8zWYbkyRtXKdwT7IXeB/wlTVKbgLuGSzfB/xRkmy+PUnSRnQ9c/8C8CngV2vsvxR4FqCqTgMvAW/ZdHeSpA1ZN9yT3Aicqqoj5ysbse2cyVmTLCZZTrK8srIyRpuSpHF0OXO/Btif5GngG8B1Sb42VHMS2AeQ5CLgzcCLwy9UVYeqqldVvbm5uU01Lkla27rhXlWfrqq9VbUA3AI8XFUfGCo7DHx4sHzzoOacM3dJ0ta4aKNfmOR2YLmqDgN3A/+Y5Dj9M/ZbJtSfJGkDxgr3qnoEeGSw/Nmztv8/4E8m2ZgkaeP8hKokNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUFdJsh+fZIfJXk8yVNJPjei5tYkK0mODh4fnU67kqQuuszE9CpwXVW9kmQX8O9JHqyqHw7V3VtVfzH5FiVJ41o33AcTXb8yWN01eDj5tSRtY52uuSe5MMlR4BTwUFU9OqLs/UmeSHJfkn0T7VKSNJZO4V5Vv6yqK4C9wFVJ3j5U8l1goareAXwPuGfU6yRZTLKcZHllZWUzfUuSzmOsu2Wq6mfAI8D1Q9tfqKpXB6tfBt61xtcfqqpeVfXm5uY20K4kqYsud8vMJbl4sPwG4D3AT4dqLjlrdT9wbJJNSpLG0+VumUuAe5JcSP+HwT9X1f1JbgeWq+ow8Ikk+4HTwIvArdNqWJK0vvRvhtl6vV6vlpeXZ3JsSdqpkhypqt56dX5CVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAZ1mYnp9Ul+lOTxJE8l+dyImtcluTfJ8SSPJlmYRrOSpG66nLm/ClxXVb8PXAFcn+TqoZo/A/6zqn4X+FvgbybbpjZsaQkWFuCCC/rPS0uz7kit8L21ra07zV71p2p6ZbC6a/AYnr7pJuB/DZbvA+5MkprVNE/qW1qCxUVYXe2vnzjRXwc4cGB2fWnn87217XW65p7kwiRHgVPAQ1X16FDJpcCzAFV1GngJeMskG9UGHDz4m2++M1ZX+9ulzfC9te11Cveq+mVVXQHsBa5K8vahkoz6suENSRaTLCdZXllZGb9bjeeZZ8bbLnXle2vbG+tumar6GfAIcP3QrpPAPoAkFwFvBl4c8fWHqqpXVb25ubkNNawxzM+Pt13qyvfWttflbpm5JBcPlt8AvAf46VDZYeDDg+WbgYe93r4N3HEH7N792m27d/e3S5vhe2vb63Lmfgnw/SRPAP+b/jX3+5PcnmT/oOZu4C1JjgN/CfzP6bSrsRw4AIcOwWWXQdJ/PnTIP3hp83xvbXuZ1Ql2r9er5eXlmRxbknaqJEeqqrdenZ9QlaQGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1qMs0e/uSfD/JsSRPJbltRM21SV5KcnTw+Ox02pUkdXFRh5rTwF9V1WNJ3gQcSfJQVf2fobp/q6obJ9+iJGlc6565V9XzVfXYYPnnwDHg0mk3JknauLGuuSdZAN4JPDpi97uTPJ7kwSRvm0BvkqQN6nJZBoAkbwS+CXyyql4e2v0YcFlVvZLkBuA7wOUjXmMRWASYn5/fcNOSpPPrdOaeZBf9YF+qqm8N76+ql6vqlcHyA8CuJHtG1B2qql5V9ebm5jbZuiRpLV3ulglwN3Csqj6/Rs1bB3UkuWrwui9MslFJUnddLstcA3wQeDLJ0cG2zwDzAFV1F3Az8LEkp4FfALdUVU2hX0lSB+uGe1X9O5B1au4E7pxUU5KkzfETqpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDWoy0xM+5J8P8mxJE8luW1ETZJ8McnxJE8kuXI67UqSuuhy5n4a+Kuq+u/A1cDHk/zeUM176U+IfTn9CbC/NNEupa2ytAQLC3DBBf3npaVZdyRtyLrhXlXPV9Vjg+WfA8eAS4fKbgK+Wn0/BC5OcsnEu5WmaWkJFhfhxAmo6j8vLhrw2pHGuuaeZAF4J/Do0K5LgWfPWj/JuT8ApO3t4EFYXX3tttXV/nZph+kc7kneCHwT+GRVvTy8e8SXnDNBdpLFJMtJlldWVsbrVJq2Z54Zb7u0jXUK9yS76Af7UlV9a0TJSWDfWet7geeGi6rqUFX1qqo3Nze3kX6l6ZmfH2+7tI11uVsmwN3Asar6/Bplh4EPDe6auRp4qaqen2Cf0vTdcQfs3v3abbt397dLO8xFHWquAT4IPJnk6GDbZ4B5gKq6C3gAuAE4DqwCH5l8q9KUHTjQfz54sH8pZn6+H+xntks7SKrOuTS+JXq9Xi0vL8/k2JK0UyU5UlW99er8hKokNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUFdptn7+ySnkvxkjf3XJnkpydHB47OTb1OSNI4u0+z9A3An8NXz1PxbVd04kY4kSZu27pl7Vf0AeHELepEkTcikrrm/O8njSR5M8rYJvaYkaYO6XJZZz2PAZVX1SpIbgO8Al48qTLIILALMz89P4NCSpFE2feZeVS9X1SuD5QeAXUn2rFF7qKp6VdWbm5vb7KElSWvYdLgneWuSDJavGrzmC5t9XUnSxq17WSbJPwHXAnuSnAT+GtgFUFV3ATcDH0tyGvgFcEtV1dQ6liSta91wr6o/XWf/nfRvlZQkbRN+QlWSGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KB1wz3J3yc5leQna+xPki8mOZ7kiSRXTr7NsywtwcICXHBB/3lpaaqHk6SdqMuZ+z8A159n/3uBywePReBLm29rDUtLsLgIJ05AVf95cdGAl6Qh64Z7Vf0AePE8JTcBX62+HwIXJ7lkUg2+xsGDsLr62m2rq/3tkqRfm8Q190uBZ89aPznYdo4ki0mWkyyvrKyMf6RnnhlvuyT9FzWJcM+IbTWqsKoOVVWvqnpzc3PjH2l+frztkvRf1CTC/SSw76z1vcBzE3jdc91xB+ze/dptu3f3t0uSfm0S4X4Y+NDgrpmrgZeq6vkJvO65DhyAQ4fgsssg6T8fOtTfLkn6tYvWK0jyT8C1wJ4kJ4G/BnYBVNVdwAPADcBxYBX4yLSaBfpBbphL0nmtG+5V9afr7C/g4xPrSJK0aX5CVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQenf7DKDAycrwIlNvMQe4D8m1M4k2dd47Ku77dgT2Ne4NtvXZVW17kf8Zxbum5Vkuap6s+5jmH2Nx7662449gX2Na6v68rKMJDXIcJekBu3kcD806wbWYF/jsa/utmNPYF/j2pK+duw1d0nS2nbymbskaQ3bOty33eTc3fu6NslLSY4OHp/dor72Jfl+kmNJnkpy24iaLR2zjj1t+XgleX2SHyV5fNDX50bUvC7JvYOxejTJwjbp69YkK2eN10en3ddZx74wyY+T3D9i35aPV8e+ZjJeSZ5O8uTgmMsj9k/3e7Gqtu0D+APgSuAna+y/AXiQ/mxQVwOPbpO+rgXun8F4XQJcOVh+E/B/gd+b5Zh17GnLx2vw3//GwfIu4FHg6qGa/wHcNVi+Bbh3m/R1K3DnVr+/Bsf+S+Dro/5/zWK8OvY1k/ECngb2nGf/VL8Xt/WZe22nybnH62smqur5qnpssPxz4Bjnzme7pWPWsactN/jvf2WwumvwGP4D1E3APYPl+4A/SjJqWsmt7msmkuwF3gd8ZY2SLR+vjn1tV1P9XtzW4d5B58m5Z+Ddg1+tH0zytq0++OBX4nfSP/M728zG7Dw9wQzGa/Cr/FHgFPBQVa05VlV1GngJeMs26Avg/YNf5e9Lsm/E/mn4AvAp4Fdr7J/JeHXoC2YzXgX8a5IjSRZH7J/q9+JOD/fOk3Nvscfof0T494G/A76zlQdP8kbgm8Anq+rl4d0jvmTqY7ZOTzMZr6r6ZVVdQX/e36uSvH2oZCZj1aGv7wILVfUO4Hv85mx5apLcCJyqqiPnKxuxbarj1bGvLR+vgWuq6krgvcDHk/zB0P6pjtdOD/etm5x7DFX18plfravqAWBXkj1bcewku+iH6FJVfWtEyZaP2Xo9zXK8Bsf8GfAIcP3Qrl+PVZKLgDezhZfj1uqrql6oqlcHq18G3rUF7VwD7E/yNPAN4LokXxuqmcV4rdvXjMaLqnpu8HwK+DZw1VDJVL8Xd3q4b93k3GNI8tYz1xqTXEV/nF/YguMGuBs4VlWfX6NsS8esS0+zGK8kc0kuHiy/AXgP8NOhssPAhwfLNwMP1+AvYbPsa+i67H76f8eYqqr6dFXtraoF+n8sfbiqPjBUtuXj1aWvWYxXkt9K8qYzy8AfA8N31031e3HdOVRnKdttcu7ufd0MfCzJaeAXwC3TfpMPXAN8EHhycM0W4DPA/Fm9bfWYdelpFuN1CXBPkgvp/zD556q6P8ntwHJVHab/Q+kfkxynfwZ6y5R76trXJ5LsB04P+rp1C/oaaRuMV5e+ZjFevwN8e3DOchHw9ar6lyR/DlvzvegnVCWpQTv9sowkaQTDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBv1/Xiu/MrcjKlYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d495420eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Visualize the dataset\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "X = [row[0] for row in dataset]\n",
    "y = [row[1] for row in dataset]\n",
    "pyplot.scatter(X, y, color='red')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our aim here is to find y using X. To do so we will start by defining mean, variance and covariance using those formulas.\n",
    "The covariance describes the relation between X and y.\n",
    "\n",
    "$${\\mu = \\frac{x_i}{n}}$$\n",
    "$${\\sigma^2 = \\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}}$$\n",
    "$${cov_{x,y}=\\frac{\\displaystyle\\sum_{i=1}^{N}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of list of values \n",
    "def mean(values):\n",
    "    return sum(values) / float(len(values))\n",
    "\n",
    "# Calculate the variance \n",
    "def variance(values, mean):\n",
    "    return sum([(x-mean)**2 for x in values])\n",
    "\n",
    "# Calculate covariance between X and y\n",
    "def covariance(X, mean_x, y, mean_y):\n",
    "    covar = 0.0\n",
    "    for i in range(len(X)):\n",
    "        covar += (X[i] - mean_x) * (y[i] - mean_y)\n",
    "    return covar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the coefficents that will help us in prediction. A simple $b$ can be calculated using \n",
    "\n",
    "$${b = \\frac{Cov\\thinspace(X,\\thinspace y)}{Var\\thinspace(X)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Coefficients\n",
    "def coefficients(dataset):\n",
    "    X = [row[0] for row in dataset]\n",
    "    y = [row[1] for row in dataset]\n",
    "    X_mean, y_mean = mean(X), mean(y)\n",
    "    b1 = covariance(X, X_mean, y, y_mean) / variance(X, X_mean)\n",
    "    b0 = y_mean - b1 * X_mean\n",
    "    return [b0, b1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Calculate RMSE\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0 \n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the algorithm that calculates $\\hat{y}$ using formula ${\\hat{y}  =  b_0  +  b_1x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression algorithm\n",
    "def simple_linear_regression(train, test):\n",
    "    predictions = list()\n",
    "    b0, b1 = coefficients(train)\n",
    "    for row in test:\n",
    "        yhat = b0 + b1 * row[0]\n",
    "        predictions.append(yhat)\n",
    "    return predictions    \n",
    "\n",
    "# Evaluate regression algo on training dataset\n",
    "def evaluate_algorithm(dataset, algorithm):\n",
    "    test_set = list()\n",
    "    for row in dataset:\n",
    "        row_copy = list(row)\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)   # remove the y and keep only x\n",
    "    predicted = algorithm(dataset, test_set)\n",
    "    print(\"Predicted values \",predicted)\n",
    "    actual = [row[-1] for row in dataset]\n",
    "    rmse = rmse_metric(actual, predicted)\n",
    "    return rmse   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our program is ready. We can test this on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values  [1.1999999999999995, 1.9999999999999996, 3.5999999999999996, 2.8, 4.3999999999999995]\n",
      "RMSE: 0.693\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluate_algorithm(dataset, simple_linear_regression)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using Statsmodels \n",
    "We can also do SRL using statsmodels library. We can fit our model using the fit() and predict using predict(). \n",
    "\n",
    "\n",
    "**NOTE:** OLS take y as first parameter and X as second parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.727\n",
      "Model:                            OLS   Adj. R-squared:                  0.636\n",
      "Method:                 Least Squares   F-statistic:                     8.000\n",
      "Date:                Sun, 08 Jul 2018   Prob (F-statistic):             0.0663\n",
      "Time:                        13:49:25   Log-Likelihood:                -5.2598\n",
      "No. Observations:                   5   AIC:                             14.52\n",
      "Df Residuals:                       3   BIC:                             13.74\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4000      0.938      0.426      0.699      -2.585       3.385\n",
      "x1             0.8000      0.283      2.828      0.066      -0.100       1.700\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   2.500\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.570\n",
      "Skew:                           0.289   Prob(JB):                        0.752\n",
      "Kurtosis:                       1.450   Cond. No.                         8.37\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "\n",
    "# Input\n",
    "X = [row[0] for row in dataset]\n",
    "\n",
    "# Output\n",
    "y = [row[1] for row in dataset]\n",
    "\n",
    "# Add b_0\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fitting model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Prediction\n",
    "predictions = model.predict(X) \n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Using Sklearn\n",
    "We can do this by using linear_model. We have also imported mean_squared_error and r2_score module to fine the MSE and r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[0.8]]\n",
      "Mean squared error: 0.48\n",
      "Variance score: 0.73\n"
     ]
    }
   ],
   "source": [
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "\n",
    "# Input\n",
    "X = np.asarray([row[0] for row in dataset]).reshape(-1, 1)\n",
    "\n",
    "# Output\n",
    "y = np.asarray([row[1] for row in dataset]).reshape(-1, 1)\n",
    "\n",
    "# define classifier\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "predictions = regr.predict(X) \n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, predictions))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: <br>\n",
    "[1] https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/ <br>\n",
    "[2] https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9 <br>\n",
    "[3] http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
